"""
AKShare å¹¶è¡Œä¸‹è½½å™¨ - å®Œæ•´ç”Ÿäº§ç‰ˆ
=================================

ç‰¹æ€§ï¼š
- å¤šè¿›ç¨‹å¹¶è¡Œä¸‹è½½ï¼ˆ4è¿›ç¨‹ï¼‰
- æ–­ç‚¹ç»­ä¼ 
- è‡ªåŠ¨é‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
- é™æµä¿æŠ¤
- è¿›åº¦ç›‘æ§
- æ•°æ®éªŒè¯
- é”™è¯¯æ—¥å¿—

ä½¿ç”¨æ–¹æ³•ï¼š
    python akshare_parallel_downloader.py

æˆ–åœ¨ä»£ç ä¸­ï¼š
    from akshare_parallel_downloader import AKShareDownloader
    
    downloader = AKShareDownloader()
    downloader.download_all()
"""

import akshare as ak
import pandas as pd
import time
import random
import logging
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp


# ==================== æ—¥å¿—é…ç½® ====================
def setup_logging(log_dir: str = "logs"):
    """é…ç½®æ—¥å¿—"""
    log_dir = Path(log_dir)
    log_dir.mkdir(exist_ok=True)
    
    log_file = log_dir / f"akshare_download_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)


# ==================== æ ¸å¿ƒä¸‹è½½å‡½æ•° ====================
def download_single_stock(
    code: str,
    start_date: str = "20140101",
    end_date: str = "20241231",
    max_retries: int = 3,
    delay_range: Tuple[float, float] = (0.2, 0.5)
) -> Tuple[str, Optional[pd.DataFrame], Optional[str]]:
    """
    ä¸‹è½½å•åªè‚¡ç¥¨ï¼ˆå­è¿›ç¨‹æ‰§è¡Œï¼‰
    
    Args:
        code: è‚¡ç¥¨ä»£ç 
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        delay_range: å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
    
    Returns:
        (code, dataframe, error_msg)
        - æˆåŠŸ: (code, df, None)
        - å¤±è´¥: (code, None, error_msg)
    """
    for attempt in range(max_retries):
        try:
            # éšæœºå»¶è¿Ÿï¼ˆé¿å…é™æµï¼‰
            time.sleep(random.uniform(*delay_range))
            
            # ä¸‹è½½æ•°æ®
            df = ak.stock_zh_a_hist(
                symbol=code,
                period="daily",
                start_date=start_date,
                end_date=end_date,
                adjust="qfq"  # å‰å¤æƒ
            )
            
            # éªŒè¯æ•°æ®
            if df is None or df.empty:
                raise ValueError("æ•°æ®ä¸ºç©º")
            
            # æ ‡å‡†åŒ–åˆ—å
            df = df.rename(columns={
                'æ—¥æœŸ': 'date',
                'å¼€ç›˜': 'open',
                'æ”¶ç›˜': 'close',
                'æœ€é«˜': 'high',
                'æœ€ä½': 'low',
                'æˆäº¤é‡': 'vol',
                'æˆäº¤é¢': 'amount',
                'æŒ¯å¹…': 'amplitude',
                'æ¶¨è·Œå¹…': 'pct_change',
                'æ¶¨è·Œé¢': 'change',
                'æ¢æ‰‹ç‡': 'turnover'
            })
            
            # æ·»åŠ è‚¡ç¥¨ä»£ç 
            df['code'] = code
            
            # æ•°æ®éªŒè¯
            if len(df) < 10:
                raise ValueError(f"æ•°æ®é‡è¿‡å°‘: {len(df)}æ¡")
            
            # æ£€æŸ¥å¿…éœ€åˆ—
            required_cols = ['date', 'open', 'high', 'low', 'close', 'vol']
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                raise ValueError(f"ç¼ºå°‘åˆ—: {missing_cols}")
            
            return (code, df, None)
        
        except Exception as e:
            error_msg = str(e)
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
            if attempt < max_retries - 1:
                # æŒ‡æ•°é€€é¿
                wait_time = 2 ** attempt
                
                # ç‰¹æ®Šå¤„ç†é™æµé”™è¯¯
                if "429" in error_msg or "é™æµ" in error_msg or "é¢‘ç¹" in error_msg:
                    wait_time = 5 * (attempt + 1)  # é™æµé”™è¯¯ç­‰å¾…æ›´ä¹…
                
                time.sleep(wait_time)
            else:
                # æœ€åä¸€æ¬¡å¤±è´¥ï¼Œè¿”å›é”™è¯¯
                return (code, None, f"ä¸‹è½½å¤±è´¥ï¼ˆå·²é‡è¯•{max_retries}æ¬¡ï¼‰: {error_msg}")
    
    return (code, None, "æœªçŸ¥é”™è¯¯")


# ==================== ä¸»ä¸‹è½½å™¨ç±» ====================
class AKShareDownloader:
    """AKShare å¹¶è¡Œä¸‹è½½å™¨"""
    
    def __init__(
        self,
        output_dir: str = "data/akshare",
        n_workers: int = 4,
        max_retries: int = 3,
        delay_range: Tuple[float, float] = (0.2, 0.5),
        start_date: str = "20140101",
        end_date: str = None
    ):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            n_workers: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆæ¨è 2-4ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            delay_range: å»¶è¿ŸèŒƒå›´ï¼ˆç§’ï¼‰
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤ä»Šå¤©ï¼‰
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.n_workers = min(n_workers, mp.cpu_count())
        self.max_retries = max_retries
        self.delay_range = delay_range
        self.start_date = start_date
        self.end_date = end_date or datetime.now().strftime("%Y%m%d")
        
        # æ—¥å¿—
        self.logger = setup_logging(self.output_dir.parent / "logs")
        
        # ç»Ÿè®¡æ–‡ä»¶
        self.stats_file = self.output_dir / "download_stats.json"
        self.failed_file = self.output_dir / "failed_stocks.txt"
    
    def get_stock_list(self) -> List[str]:
        """è·å–è‚¡ç¥¨åˆ—è¡¨"""
        self.logger.info("æ­£åœ¨è·å–è‚¡ç¥¨åˆ—è¡¨...")
        
        try:
            # è·å–Aè‚¡åˆ—è¡¨
            stock_info = ak.stock_info_a_code_name()
            codes = stock_info['code'].tolist()
            
            self.logger.info(f"âœ“ è·å–åˆ° {len(codes)} åªè‚¡ç¥¨")
            
            return codes
        
        except Exception as e:
            self.logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
            raise
    
    def get_downloaded_codes(self) -> set:
        """è·å–å·²ä¸‹è½½çš„è‚¡ç¥¨ä»£ç """
        downloaded = set()
        
        # æ£€æŸ¥ parquet æ–‡ä»¶
        for f in self.output_dir.glob("*.parquet"):
            downloaded.add(f.stem)
        
        # æ£€æŸ¥ csv æ–‡ä»¶ï¼ˆå…¼å®¹ï¼‰
        for f in self.output_dir.glob("*.csv"):
            downloaded.add(f.stem)
        
        return downloaded
    
    def save_stock_data(self, code: str, df: pd.DataFrame) -> bool:
        """
        ä¿å­˜è‚¡ç¥¨æ•°æ®
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: æ•°æ®
        
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä¿å­˜ä¸º parquetï¼ˆæ¨èï¼Œä½“ç§¯å°ã€é€Ÿåº¦å¿«ï¼‰
            output_path = self.output_dir / f"{code}.parquet"
            df.to_parquet(output_path, index=False)
            
            return True
        
        except Exception as e:
            self.logger.error(f"ä¿å­˜ {code} å¤±è´¥: {e}")
            return False
    
    def save_statistics(self, stats: Dict):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
    
    def save_failed_list(self, failed: List[Tuple[str, str]]):
        """ä¿å­˜å¤±è´¥åˆ—è¡¨"""
        try:
            with open(self.failed_file, 'w', encoding='utf-8') as f:
                for code, error in failed:
                    f.write(f"{code}\t{error}\n")
        except Exception as e:
            self.logger.error(f"ä¿å­˜å¤±è´¥åˆ—è¡¨å¤±è´¥: {e}")
    
    def download_batch(
        self,
        stock_codes: List[str],
        resume: bool = True
    ) -> Dict:
        """
        æ‰¹é‡ä¸‹è½½
        
        Args:
            stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        # æ–­ç‚¹ç»­ä¼ 
        if resume:
            downloaded = self.get_downloaded_codes()
            pending = [c for c in stock_codes if c not in downloaded]
            
            self.logger.info(f"âœ“ å·²ä¸‹è½½: {len(downloaded)} åª")
            self.logger.info(f"â³ å¾…ä¸‹è½½: {len(pending)} åª")
            
            if not pending:
                self.logger.info("ğŸ‰ å…¨éƒ¨å·²ä¸‹è½½ï¼")
                return {
                    'total': len(stock_codes),
                    'downloaded': len(downloaded),
                    'pending': 0,
                    'success': 0,
                    'failed': 0
                }
        else:
            pending = stock_codes
        
        # å¼€å§‹ä¸‹è½½
        self.logger.info("=" * 70)
        self.logger.info("å¼€å§‹å¹¶è¡Œä¸‹è½½")
        self.logger.info("=" * 70)
        self.logger.info(f"æ€»æ•°: {len(pending)} åª")
        self.logger.info(f"è¿›ç¨‹æ•°: {self.n_workers}")
        self.logger.info(f"æ—¥æœŸèŒƒå›´: {self.start_date} - {self.end_date}")
        self.logger.info("=" * 70)
        
        success_count = 0
        failed_list = []
        
        t0 = time.time()
        
        # å¤šè¿›ç¨‹ä¸‹è½½
        with ProcessPoolExecutor(max_workers=self.n_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(
                    download_single_stock,
                    code,
                    self.start_date,
                    self.end_date,
                    self.max_retries,
                    self.delay_range
                ): code for code in pending
            }
            
            # æ”¶é›†ç»“æœ
            completed = 0
            
            for future in as_completed(futures):
                code, df, error = future.result()
                
                if df is not None:
                    # ä¿å­˜æ•°æ®
                    if self.save_stock_data(code, df):
                        success_count += 1
                    else:
                        failed_list.append((code, "ä¿å­˜å¤±è´¥"))
                else:
                    failed_list.append((code, error))
                    self.logger.warning(f"âœ— {code}: {error}")
                
                completed += 1
                
                # æ˜¾ç¤ºè¿›åº¦
                if completed % 50 == 0 or completed == len(pending):
                    elapsed = time.time() - t0
                    rate = completed / elapsed if elapsed > 0 else 0
                    eta = (len(pending) - completed) / rate if rate > 0 else 0
                    
                    self.logger.info(
                        f"è¿›åº¦: {completed}/{len(pending)} "
                        f"({completed/len(pending)*100:.1f}%) | "
                        f"æˆåŠŸ: {success_count} | "
                        f"å¤±è´¥: {len(failed_list)} | "
                        f"é€Ÿåº¦: {rate:.2f}è‚¡/ç§’ | "
                        f"ETA: {eta/60:.1f}åˆ†é’Ÿ"
                    )
        
        # ç»Ÿè®¡
        elapsed = time.time() - t0
        
        stats = {
            'total': len(stock_codes),
            'downloaded': len(self.get_downloaded_codes()) - len(pending) + success_count,
            'pending': len(pending),
            'success': success_count,
            'failed': len(failed_list),
            'elapsed_seconds': round(elapsed, 2),
            'elapsed_minutes': round(elapsed / 60, 2),
            'rate': round(len(pending) / elapsed if elapsed > 0 else 0, 2),
            'start_time': datetime.fromtimestamp(t0).isoformat(),
            'end_time': datetime.now().isoformat()
        }
        
        # ä¿å­˜ç»Ÿè®¡
        self.save_statistics(stats)
        
        # ä¿å­˜å¤±è´¥åˆ—è¡¨
        if failed_list:
            self.save_failed_list(failed_list)
        
        # æ‰“å°æ±‡æ€»
        self.logger.info("\n" + "=" * 70)
        self.logger.info("ä¸‹è½½å®Œæˆï¼")
        self.logger.info("=" * 70)
        self.logger.info(f"âœ“ æˆåŠŸ: {success_count}/{len(pending)}")
        self.logger.info(f"âœ— å¤±è´¥: {len(failed_list)}")
        self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        self.logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {len(pending)/elapsed:.2f} è‚¡/ç§’")
        self.logger.info("=" * 70)
        
        if failed_list:
            self.logger.info(f"\nå¤±è´¥åˆ—è¡¨å·²ä¿å­˜åˆ°: {self.failed_file}")
            self.logger.info(f"å‰10ä¸ªå¤±è´¥è‚¡ç¥¨:")
            for code, error in failed_list[:10]:
                self.logger.info(f"  - {code}: {error}")
            if len(failed_list) > 10:
                self.logger.info(f"  ... è¿˜æœ‰ {len(failed_list)-10} åª")
        
        return stats
    
    def download_all(self, resume: bool = True) -> Dict:
        """
        ä¸‹è½½å…¨éƒ¨è‚¡ç¥¨
        
        Args:
            resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–è‚¡ç¥¨åˆ—è¡¨
        stock_codes = self.get_stock_list()
        
        # æ‰¹é‡ä¸‹è½½
        stats = self.download_batch(stock_codes, resume=resume)
        
        return stats
    
    def retry_failed(self) -> Dict:
        """
        é‡è¯•å¤±è´¥çš„è‚¡ç¥¨
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯
        """
        if not self.failed_file.exists():
            self.logger.info("æ²¡æœ‰å¤±è´¥åˆ—è¡¨")
            return {}
        
        # è¯»å–å¤±è´¥åˆ—è¡¨
        failed_codes = []
        with open(self.failed_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    code = line.split('\t')[0]
                    failed_codes.append(code)
        
        self.logger.info(f"é‡è¯• {len(failed_codes)} åªå¤±è´¥è‚¡ç¥¨...")
        
        # é‡è¯•
        stats = self.download_batch(failed_codes, resume=False)
        
        return stats
    
    def verify_data(self, sample_size: int = 100) -> Dict:
        """
        éªŒè¯æ•°æ®å®Œæ•´æ€§
        
        Args:
            sample_size: æŠ½æ ·æ•°é‡
        
        Returns:
            éªŒè¯ç»“æœ
        """
        self.logger.info(f"éªŒè¯æ•°æ®å®Œæ•´æ€§ï¼ˆæŠ½æ · {sample_size} åªï¼‰...")
        
        # è·å–å·²ä¸‹è½½æ–‡ä»¶
        files = list(self.output_dir.glob("*.parquet"))
        
        if not files:
            self.logger.warning("æ²¡æœ‰æ‰¾åˆ°æ•°æ®æ–‡ä»¶")
            return {}
        
        # éšæœºæŠ½æ ·
        import random
        sample_files = random.sample(files, min(sample_size, len(files)))
        
        issues = []
        
        for f in sample_files:
            try:
                df = pd.read_parquet(f)
                
                # æ£€æŸ¥1: æ•°æ®é‡
                if len(df) < 10:
                    issues.append((f.stem, f"æ•°æ®é‡è¿‡å°‘: {len(df)}æ¡"))
                
                # æ£€æŸ¥2: å¿…éœ€åˆ—
                required_cols = ['date', 'open', 'high', 'low', 'close', 'vol']
                missing = [col for col in required_cols if col not in df.columns]
                if missing:
                    issues.append((f.stem, f"ç¼ºå°‘åˆ—: {missing}"))
                
                # æ£€æŸ¥3: ç¼ºå¤±å€¼
                null_cols = df[required_cols].isnull().sum()
                if null_cols.any():
                    issues.append((f.stem, f"æœ‰ç¼ºå¤±å€¼: {null_cols[null_cols > 0].to_dict()}"))
                
            except Exception as e:
                issues.append((f.stem, f"è¯»å–å¤±è´¥: {e}"))
        
        # æŠ¥å‘Š
        self.logger.info(f"âœ“ éªŒè¯å®Œæˆ: {len(sample_files)} ä¸ªæ–‡ä»¶")
        
        if issues:
            self.logger.warning(f"å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
            for code, issue in issues[:10]:
                self.logger.warning(f"  - {code}: {issue}")
        else:
            self.logger.info("âœ“ æ‰€æœ‰æŠ½æ ·æ–‡ä»¶å‡æ­£å¸¸")
        
        return {
            'total_checked': len(sample_files),
            'issues_found': len(issues),
            'issues': issues
        }


# ==================== å‘½ä»¤è¡Œæ¥å£ ====================
def main():
    """å‘½ä»¤è¡Œä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="AKShare å¹¶è¡Œä¸‹è½½å™¨")
    
    parser.add_argument(
        '--output-dir',
        default='data/akshare',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: data/akshareï¼‰'
    )
    
    parser.add_argument(
        '--workers',
        type=int,
        default=4,
        help='å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé»˜è®¤: 4ï¼‰'
    )
    
    parser.add_argument(
        '--start-date',
        default='20140101',
        help='å¼€å§‹æ—¥æœŸï¼ˆé»˜è®¤: 20140101ï¼‰'
    )
    
    parser.add_argument(
        '--end-date',
        default=None,
        help='ç»“æŸæ—¥æœŸï¼ˆé»˜è®¤: ä»Šå¤©ï¼‰'
    )
    
    parser.add_argument(
        '--no-resume',
        action='store_true',
        help='ä¸ä½¿ç”¨æ–­ç‚¹ç»­ä¼ ï¼ˆé‡æ–°ä¸‹è½½ï¼‰'
    )
    
    parser.add_argument(
        '--retry-failed',
        action='store_true',
        help='é‡è¯•å¤±è´¥çš„è‚¡ç¥¨'
    )
    
    parser.add_argument(
        '--verify',
        action='store_true',
        help='éªŒè¯æ•°æ®å®Œæ•´æ€§'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = AKShareDownloader(
        output_dir=args.output_dir,
        n_workers=args.workers,
        start_date=args.start_date,
        end_date=args.end_date
    )
    
    # æ‰§è¡Œæ“ä½œ
    if args.retry_failed:
        # é‡è¯•å¤±è´¥
        downloader.retry_failed()
    
    elif args.verify:
        # éªŒè¯æ•°æ®
        downloader.verify_data()
    
    else:
        # æ­£å¸¸ä¸‹è½½
        downloader.download_all(resume=not args.no_resume)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================
if __name__ == "__main__":
    # æ–¹å¼1: å‘½ä»¤è¡Œä½¿ç”¨
    # python akshare_parallel_downloader.py
    
    # æ–¹å¼2: ä»£ç è°ƒç”¨
    """
    downloader = AKShareDownloader(
        output_dir="data/akshare",
        n_workers=4,
        start_date="20140101"
    )
    
    # ä¸‹è½½å…¨éƒ¨ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    stats = downloader.download_all(resume=True)
    
    # é‡è¯•å¤±è´¥
    downloader.retry_failed()
    
    # éªŒè¯æ•°æ®
    downloader.verify_data()
    """
    
    main()
